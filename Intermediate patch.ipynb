{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP2GvKZIL9fGp2FuKOyocms"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"MmS3Nvue62Ja"},"outputs":[],"source":["import numpy as np\n","import os\n","import torch\n","#from neuromaps.datasets import fetch_fsaverage\n","#from surfplot import Plot\n","import scipy.io\n","import warnings\n","from nilearn import datasets\n","from nilearn import plotting\n","from PIL import Image\n","\n","import matplotlib.pyplot as plt\n","\n","from spacetorch.feature_extractor import FeatureExtractor\n","from torch.utils.data import Dataset, DataLoader\n","from spacetorch.datasets import DatasetRegistry\n","from einops import reduce, rearrange\n","from load_model import get_closest_factors\n","\n","device = ('cuda' if torch.cuda.is_available() else 'cpu')\n","from torch.utils.data import DataLoader, Dataset\n","\n","from pathlib import Path\n","from torchvision.transforms import transforms\n","transform = transforms.Compose([\n","    transforms.Resize((224,224)),\n","    transforms.ToTensor(),\n","    #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","])\n","\n","from load_model import *\n","from spacetorch.feature_extractor import FeatureExtractor\n","import torchvision\n"]},{"cell_type":"markdown","source":["# Correlation"],"metadata":{"id":"xnzVF6w171k4"}},{"cell_type":"code","source":["\n","DUMMY_LABEL = 0\n","\n","class ImageDataset(Dataset):\n","    def __init__(self, imgs_paths, idxs, transform):\n","        self.imgs_paths = np.array(imgs_paths)[idxs]\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.imgs_paths)\n","\n","    def __getitem__(self, idx):\n","        # Load the image\n","        img_path = self.imgs_paths[idx]\n","        img = Image.open(img_path).convert('RGB')\n","        if self.transform:\n","            img = self.transform(img).to(device)\n","        return img,DUMMY_LABEL\n","\n","\n","\n","\n","\n","\n","def ROI_selection(hemisphere, roi):\n","    subject_dir = '/home/xinyuq/projects/def-bashivan/xinyuq/kernel_avpool/data/datasets/algonauts_2023_tutorial_data/subj04'\n","    # Define the ROI class based on the selected ROI\n","    if roi in [\"V1v\", \"V1d\", \"V2v\", \"V2d\", \"V3v\", \"V3d\", \"hV4\"]:\n","        roi_class = 'prf-visualrois'\n","    elif roi in [\"EBA\", \"FBA-1\", \"FBA-2\", \"mTL-bodies\"]:\n","        roi_class = 'floc-bodies'\n","    elif roi in [\"OFA\", \"FFA-1\", \"FFA-2\", \"mTL-faces\", \"aTL-faces\"]:\n","        roi_class = 'floc-faces'\n","    elif roi in [\"OPA\", \"PPA\", \"RSC\"]:\n","        roi_class = 'floc-places'\n","    elif roi in [\"OWFA\", \"VWFA-1\", \"VWFA-2\", \"mfs-words\", \"mTL-words\"]:\n","        roi_class = 'floc-words'\n","    elif roi in [\"early\", \"midventral\", \"midlateral\", \"midparietal\", \"ventral\", \"lateral\", \"parietal\"]:\n","        roi_class = 'streams'\n","\n","    # Load the ROI brain surface maps\n","    roi_class_dir = os.path.join(subject_dir, 'roi_masks', hemisphere[0]+'h.'+roi_class+'_fsaverage_space.npy')\n","    roi_map_dir = os.path.join(subject_dir, 'roi_masks','mapping_'+roi_class+'.npy')\n","    fsaverage_roi_class = np.load(roi_class_dir)\n","    roi_map = np.load(roi_map_dir, allow_pickle=True).item()\n","\n","    roi_mapping = list(roi_map.keys())[list(roi_map.values()).index(roi)]\n","    fsaverage_roi = np.asarray(fsaverage_roi_class == roi_mapping, dtype=int)\n","    return fsaverage_roi\n","\n","\n","def point_inside_polygon(x, y, poly):\n","    n = len(poly)\n","    inside = False\n","    p1x, p1y = poly[0]\n","    for i in range(n + 1):\n","        p2x, p2y = poly[i % n]\n","        if y > min(p1y, p2y):\n","            if y <= max(p1y, p2y):\n","                if x <= max(p1x, p2x):\n","                    if p1y != p2y:\n","                        xinters = (y - p1y) * (p2x - p1x) / (p2y - p1y) + p1x\n","                    if p1x == p2x or x <= xinters:\n","                        inside = not inside\n","        p1x, p1y = p2x, p2y\n","    return inside\n","\n","\n","def select_intermediate_patch(hemisphere,roi_1,roi_2,rectangle_size_x=30,rectangle_size_y=5,adjust_x_1=0,adjust_y_1=0):\n","\n","    path_info = \"/home/xinyuq/projects/def-bashivan/xinyuq/kernel_avpool/data/datasets/algonauts_2023_tutorial_data/\"\n","    subject_dir =  os.path.join(path_info, 'subj04/')\n","    subject_info = os.path.join(subject_dir, 'roi_masks/')\n","\n","    fsaverage_roi_patch1 = ROI_selection(hemisphere, roi_1)\n","    fsaverage_roi_patch2 = ROI_selection(hemisphere, roi_2)\n","\n","\n","    rh_fsaverage_x = scipy.io.loadmat(path_info+f'{hemisphere[0]}h_x.mat')[f'x_{hemisphere[0]}h']\n","    rh_fsaverage_y = scipy.io.loadmat(path_info+f'{hemisphere[0]}h_y.mat')[f'y_{hemisphere[0]}h']\n","    rh_fsaverage_ind = scipy.io.loadmat(path_info+f'{hemisphere[0]}h_ind.mat')[f'ind_{hemisphere[0]}h']\n","\n","    temp_rh = np.zeros((len(fsaverage_roi_patch1),1))\n","    temp_rh[rh_fsaverage_ind] = 1\n","    temp_rh = temp_rh.reshape(len(temp_rh),)\n","\n","    X_val = np.full((len(fsaverage_roi_patch1),), np.nan)\n","    Y_val = np.full((len(fsaverage_roi_patch1),), np.nan)\n","\n","    X_val[temp_rh == 1,] = rh_fsaverage_x.reshape(np.size(rh_fsaverage_x,axis=1),)\n","    Y_val[temp_rh == 1,] = rh_fsaverage_y.reshape(np.size(rh_fsaverage_y,axis=1),)\n","\n","    rh_patch1_data_with_xy = np.full((len(fsaverage_roi_patch1),3), np.nan)# 0: data; 1: x value; 2: y_value\n","    rh_patch1_data_with_xy[:,0] = fsaverage_roi_patch1\n","    rh_patch1_data_with_xy[:,1] = X_val\n","    rh_patch1_data_with_xy[:,2] = Y_val\n","\n","    rh_patch2_data_with_xy = np.full((len(fsaverage_roi_patch2),3), np.nan)# 0: data; 1: x value; 2: y_value\n","    rh_patch2_data_with_xy[:,0] = fsaverage_roi_patch2\n","    rh_patch2_data_with_xy[:,1] = X_val\n","    rh_patch2_data_with_xy[:,2] = Y_val\n","\n","    # patch 1\n","    # Masking the values based on the first dimension\n","    mask = rh_patch1_data_with_xy[:, 0] == 1\n","    # Apply the mask to your data\n","    filtered_data = rh_patch1_data_with_xy[mask]\n","    # Reshape the filtered data to match the new shape\n","    new_shape = (filtered_data.shape[0],) + rh_patch1_data_with_xy.shape[1:]\n","    data_patch1 = filtered_data.reshape(new_shape)\n","    print('data1', data_patch1.shape)\n","\n","    # patch2\n","    # Masking the values based on the first dimension\n","    mask = rh_patch2_data_with_xy[:, 0] == 1\n","    # Apply the mask to your data\n","    filtered_data = rh_patch2_data_with_xy[mask]\n","    # Reshape the filtered data to match the new shape\n","    new_shape = (filtered_data.shape[0],) + rh_patch2_data_with_xy.shape[1:]\n","    data_patch2 = filtered_data.reshape(new_shape)\n","\n","    # Find the mean x and y of the two patches\n","    X_patch1_mean = np.nanmean(data_patch1[:,1])\n","    Y_patch1_mean = np.nanmean(data_patch1[:,2])\n","    X_patch2_mean = np.nanmean(data_patch2[:,1])\n","    Y_patch2_mean = np.nanmean(data_patch2[:,2])\n","    ###############################################################################\n","    # Define the coordinates of the two patches\n","    patch1_x, patch1_y = X_patch1_mean, Y_patch1_mean\n","    patch2_x, patch2_y = X_patch2_mean, Y_patch2_mean\n","\n","    # Calculate the direction vector of the line connecting the two points\n","    direction_vector = np.array([patch2_x - patch1_x, patch2_y - patch1_y])\n","    direction_vector /= np.linalg.norm(direction_vector)  # Normalize the direction vector\n","\n","    # Calculate the coordinates of the square's vertices based on the direction vector\n","    center_x = (patch1_x + patch2_x) / 2\n","    center_y = (patch1_y + patch2_y) / 2\n","\n","\n","    offset_vector_x = rectangle_size_x / 2 * np.array([direction_vector[1], direction_vector[0]])\n","    offset_vector_y = rectangle_size_y / 2 * np.array([-direction_vector[0], direction_vector[1]])\n","\n","    # Calculate the coordinates of the rectangle's vertices\n","    rectangle_vertices = [\n","        [center_x + offset_vector_x[0] + offset_vector_y[0] - adjust_x_1, center_y + offset_vector_x[1] + offset_vector_y[1] - adjust_y_1],\n","        [center_x - offset_vector_x[0] + offset_vector_y[0] - adjust_x_1, center_y - offset_vector_x[1] + offset_vector_y[1]- adjust_y_1],\n","        [center_x - offset_vector_x[0] - offset_vector_y[0] - adjust_x_1, center_y - offset_vector_x[1] - offset_vector_y[1]- adjust_y_1],\n","        [center_x + offset_vector_x[0] - offset_vector_y[0] - adjust_x_1, center_y + offset_vector_x[1] - offset_vector_y[1]- adjust_y_1]\n","    ]\n","    rectangle_vertices = np.array(rectangle_vertices)\n","    ###############################################################################\n","    # Generate a grid of points within the rectangle's bounding box\n","    x_grid = np.linspace(min(rectangle_vertices[:, 0]), max(rectangle_vertices[:, 0]), num=100)\n","    y_grid = np.linspace(min(rectangle_vertices[:, 1]), max(rectangle_vertices[:, 1]), num=100)\n","    X, Y = np.meshgrid(x_grid, y_grid)\n","\n","    points_within_rectangle = np.column_stack((X_val.flatten(),Y_val.flatten()))\n","\n","    filtered_points = [point for point in points_within_rectangle if point_inside_polygon(point[0], point[1], rectangle_vertices)]\n","    filtered_points = np.array(filtered_points)\n","\n","    ROIs = rh_patch1_data_with_xy + rh_patch2_data_with_xy\n","    other_patches = rh_patch1_data_with_xy + rh_patch2_data_with_xy\n","\n","    intermediate_patch = np.zeros(ROIs.shape)\n","    for i in range(len(filtered_points)):\n","        # Find indices where filtered point matches X_val and Y_val\n","        matching_indices = np.where((filtered_points[i, 0] == X_val) & (filtered_points[i, 1] == Y_val))[0]\n","        # Set the corresponding rows in mask_rect to 2\n","        ROIs[matching_indices, :] = 2\n","        intermediate_patch[matching_indices, :] = 2\n","\n","    return ROIs,intermediate_patch,other_patches,fsaverage_roi_patch1,fsaverage_roi_patch2\n","\n","\n","\n","def get_intermediate_fMRI(hemisphere,ROI_fsaverage, subject_dir, length, fmri_dir):\n","\n","    roi_dir = os.path.join(subject_dir, 'roi_masks', hemisphere[0] + 'h.all-vertices_fsaverage_space.npy')\n","\n","    fsaverage_all_vertices = np.load(roi_dir)\n","\n","    temp =  np.zeros([length, fsaverage_all_vertices.shape[0]])\n","    #print(len(train_img_list))\n","    fmri_value = np.load(os.path.join(fmri_dir, f'{hemisphere[0]}h_training_fmri.npy'))\n","\n","    valid_indices = np.where(fsaverage_all_vertices == 1)[0]\n","    #print(temp.shape)\n","    #print(valid_indices.shape)\n","    #print(fmri_value.shape)\n","    temp[:, valid_indices] = fmri_value[:, :len(valid_indices)]\n","\n","    roi_idx = np.where(ROI_fsaverage)[0]\n","    fmri_value = temp[:,roi_idx]\n","    print(fmri_value.shape)\n","    return fmri_value\n","\n","\n","\n","def plot_correlations(features,fMRI_value, patch):\n","\n","    num_channles = features.shape[1]\n","    avg_feature_activations = features #np.mean(features, axis=(2,3)) # Shape: (529, 7, 7)\n","\n","    # Calculate the average fMRI data across images\n","    avg_fMRI_data = np.mean(fMRI_value,axis=(1))  # Shape: (266,)\n","\n","    correlations = np.zeros(num_channles)\n","\n","    for channel in range(num_channles):\n","        correlations[channel] = np.corrcoef(\n","            avg_feature_activations[:,channel],\n","            avg_fMRI_data\n","        )[0, 1]\n","    x_dim,y_dim = get_closest_factors(num_channles)\n","    x_dim = int(x_dim/2)\n","    y_dim = int(y_dim*2)\n","\n","    correlation_grid = correlations.reshape(y_dim, x_dim)\n","\n","    plt.figure(figsize=(10, 40))\n","    #plt, ax = plt.subplots(figsize=(10, 40))\n","    heatmap = plt.imshow(correlation_grid.reshape(y_dim,x_dim), cmap='coolwarm', interpolation='nearest',vmin=-0.5, vmax=0.5, aspect=\"auto\")\n","    plt.colorbar(heatmap, aspect=40)\n","    plt.title(f'unit correlation for {patch} patch')\n","\n","    plt.xticks(range(x_dim))\n","    plt.yticks(range(y_dim))\n","    plt.show()\n","    #plt.gca().spines[['left','bottom','right', 'top']].set_visible(False)\n","    plt.gca().axes.get_xaxis().set_visible(False)\n","    plt.gca().axes.get_yaxis().set_visible(False)\n","\n","    plt.savefig(f\"inter/{patch}.png\")#, format='pdf')\n","    plt.savefig(f\"inter/{patch}.pdf\", format='pdf')\n","    plt.clf()\n","\n","    top_52_indices = np.unravel_index(np.argsort(correlation_grid, axis=None)[-10:], correlation_grid.shape)\n","\n","    top_52_matrix = np.zeros_like(correlation_grid)\n","\n","    for index in zip(*top_52_indices):\n","        top_52_matrix[index] = 1\n","    return top_52_matrix.reshape(y_dim, x_dim)\n","\n"],"metadata":{"id":"6CQLH-bZ7q6D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","def main():\n","\n","    hemisphere = 'right'\n","    #ROIs_2,intermediate_2,other_patches, f1, f2 = select_intermediate_patch(hemisphere=hemisphere,roi_1='FFA-2',roi_2='PPA',rectangle_size_x=5,rectangle_size_y=5,adjust_x_1=5,adjust_y_1=0)\n","    ROIs_2,intermediate_2,other_patches, f1, f2 = select_intermediate_patch(hemisphere=hemisphere,roi_1='FFA-2',roi_2='PPA',rectangle_size_x=5,rectangle_size_y=5,adjust_x_1=0,adjust_y_1=0)\n","\n","\n","    fmri_dir = os.path.join('/home/xinyuq/projects/def-bashivan/xinyuq/kernel_avpool/data/datasets/algonauts_2023_tutorial_data/subj04', 'training_split', 'training_fmri')\n","    lh_fmri = np.load(os.path.join(fmri_dir, 'lh_training_fmri.npy'))\n","    rh_fmri = np.load(os.path.join(fmri_dir, 'rh_training_fmri.npy'))\n","    path_info = \"/home/xinyuq/projects/def-bashivan/xinyuq/kernel_avpool/data/datasets/algonauts_2023_tutorial_data/\"\n","    subject_dir =  os.path.join(path_info, 'subj04/')\n","    subject_info = os.path.join(subject_dir, 'roi_masks/')\n","\n","    train_img_dir  = os.path.join(subject_dir, 'training_split', 'training_images')\n","\n","    # Create lists will all training and test image file names, sorted\n","    train_img_list = os.listdir(train_img_dir)\n","    train_img_list.sort()\n","\n","\n","    batch_size = 300\n","    train_imgs_paths = sorted(list(Path(train_img_dir).iterdir()))\n","\n","     # Calculate how many stimulus images correspond to 90% of the training data\n","    num_train = int(np.round(len(train_img_list) / 100 * 90))\n","    # Shuffle all training stimulus images\n","    idxs = np.arange(len(train_img_list))\n","    np.random.shuffle(idxs)\n","    # Assign 90% of the shuffled stimulus images to the training partition,\n","    # and 10% to the test partition\n","    idxs_train, idxs_val = idxs[:num_train], idxs[num_train:]\n","\n","\n","    intermediate_fMRI= get_intermediate_fMRI( hemisphere=hemisphere,ROI_fsaverage=intermediate_2[:,0], subject_dir=subject_dir, length=len(train_img_list), fmri_dir=fmri_dir)[idxs_train]\n","\n","    print('fmri', intermediate_fMRI)\n","    face_fMRI= get_intermediate_fMRI( hemisphere=hemisphere,ROI_fsaverage=f1, subject_dir=subject_dir, length=len(train_img_list), fmri_dir=fmri_dir)[idxs_train]\n","    place_fMRI= get_intermediate_fMRI( hemisphere=hemisphere,ROI_fsaverage=f2, subject_dir=subject_dir, length=len(train_img_list), fmri_dir=fmri_dir)[idxs_train]\n","\n","\n","\n","\n","\n","    train_imgs_dataloader = DataLoader(\n","        ImageDataset(train_imgs_paths, idxs_train, transform),\n","        batch_size=batch_size\n","    )\n","\n","    model = load_model(pool_type='gaussian', kap_kernelsize=0.23, continuous=True, local_conv=False, expname='gaussian_0.23_continuous_prog_t', epoch=100, sel_range=10)\n","    #features= FeatureExtractor(train_imgs_dataloader, 32).extract_features(model, [model.layer4[0].bn1, model.layer4[0].bn2, model.layer4[1].bn1, model.layer4[1].bn2], return_inputs_and_labels=False)\n","    features= FeatureExtractor(train_imgs_dataloader, 32).extract_features(model, [model.layer4[0].conv1, model.layer4[0].conv2, model.layer4[1].conv1, model.layer4[1].conv2], return_inputs_and_labels=False)\n","    #features= FeatureExtractor(train_imgs_dataloader, 32).extract_features(model, [model.layer3[0].conv1, model.layer3[0].conv2, model.layer3[1].conv1, model.layer3[1].conv2], return_inputs_and_labels=False)\n","    features_1, features_2, features_3, features_4 = features\n","    avg_features_1 = reduce(features_1, 'b c h w -> b c', 'mean')\n","    avg_features_2 = reduce(features_2, 'b c h w -> b c', 'mean')\n","    avg_features_3 = reduce(features_3, 'b c h w -> b c', 'mean')\n","    avg_features_4 = reduce(features_4, 'b c h w -> b c', 'mean')\n","\n","    avg_features = np.concatenate((avg_features_1, avg_features_2, avg_features_3, avg_features_4), axis=1)\n","\n","    top_52_matrix_3_f = plot_correlations(features=avg_features,fMRI_value=face_fMRI, patch='face')\n","    top_52_matrix_3_inter = plot_correlations(features=avg_features,fMRI_value=intermediate_fMRI, patch='intermediate')\n","    top_52_matrix_3_p = plot_correlations(features=avg_features,fMRI_value=place_fMRI, patch='place')\n","\n","    three_in_one(top_52_matrix_3_inter,top_52_matrix_3_f, top_52_matrix_3_p)\n",""],"metadata":{"id":"LWb0HtDe8Iuf"},"execution_count":null,"outputs":[]}]}